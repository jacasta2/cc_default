{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "I first perform some data processing where I transform, scale and one-hot encode several features. Then, I train, validate and test a logistic regression model and a XGBoost classifier. I also do some manual hyperparameter fine-tuning to address overfitting in the XGBoost classifier.\n",
    "\n",
    "I proceed with a k-fold cross-validation using the models default implementation and compare their performance. Finally, I perform a hyperparameter tuning using cross-validation and compare the performance of the resulting models using a validation set.\n",
    "\n",
    "I provide some basic comments within some code cells to follow what the code does."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict, \\\n",
    "    GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   ID                          30000 non-null  int64  \n",
      " 1   LIMIT_BAL                   30000 non-null  float64\n",
      " 2   SEX                         30000 non-null  int64  \n",
      " 3   EDUCATION                   30000 non-null  int64  \n",
      " 4   MARRIAGE                    30000 non-null  int64  \n",
      " 5   AGE                         30000 non-null  int64  \n",
      " 6   PAY_0                       30000 non-null  int64  \n",
      " 7   PAY_2                       30000 non-null  int64  \n",
      " 8   PAY_3                       30000 non-null  int64  \n",
      " 9   PAY_4                       30000 non-null  int64  \n",
      " 10  PAY_5                       30000 non-null  int64  \n",
      " 11  PAY_6                       30000 non-null  int64  \n",
      " 12  BILL_AMT1                   30000 non-null  float64\n",
      " 13  BILL_AMT2                   30000 non-null  float64\n",
      " 14  BILL_AMT3                   30000 non-null  float64\n",
      " 15  BILL_AMT4                   30000 non-null  float64\n",
      " 16  BILL_AMT5                   30000 non-null  float64\n",
      " 17  BILL_AMT6                   30000 non-null  float64\n",
      " 18  PAY_AMT1                    30000 non-null  float64\n",
      " 19  PAY_AMT2                    30000 non-null  float64\n",
      " 20  PAY_AMT3                    30000 non-null  float64\n",
      " 21  PAY_AMT4                    30000 non-null  float64\n",
      " 22  PAY_AMT5                    30000 non-null  float64\n",
      " 23  PAY_AMT6                    30000 non-null  float64\n",
      " 24  default.payment.next.month  30000 non-null  int64  \n",
      "dtypes: float64(13), int64(12)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "### The DataFrame is wide, so we'd like to be able to see all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "### Let's load the data into a DataFrame and check some basic info\n",
    "creditCard = pd.read_csv(\"../UCI_Credit_Card.csv\")\n",
    "creditCard.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_1</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>DEFAULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>3913.0</td>\n",
       "      <td>3102.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29239.0</td>\n",
       "      <td>14027.0</td>\n",
       "      <td>13559.0</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46990.0</td>\n",
       "      <td>48233.0</td>\n",
       "      <td>49291.0</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8617.0</td>\n",
       "      <td>5670.0</td>\n",
       "      <td>35835.0</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_1  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0     -2     -2     3913.0     3102.0      689.0        0.0        0.0   \n",
       "1      0      2     2682.0     1725.0     2682.0     3272.0     3455.0   \n",
       "2      0      0    29239.0    14027.0    13559.0    14331.0    14948.0   \n",
       "3      0      0    46990.0    48233.0    49291.0    28314.0    28959.0   \n",
       "4      0      0     8617.0     5670.0    35835.0    20940.0    19146.0   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0        0.0       0.0     689.0       0.0       0.0       0.0       0.0   \n",
       "1     3261.0       0.0    1000.0    1000.0    1000.0       0.0    2000.0   \n",
       "2    15549.0    1518.0    1500.0    1000.0    1000.0    1000.0    5000.0   \n",
       "3    29547.0    2000.0    2019.0    1200.0    1100.0    1069.0    1000.0   \n",
       "4    19131.0    2000.0   36681.0   10000.0    9000.0     689.0     679.0   \n",
       "\n",
       "   DEFAULT  \n",
       "0        1  \n",
       "1        1  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We'll rename 'default.payment.next.month' to 'DEFAULT' for simplicity\n",
    "creditCard.rename({\"default.payment.next.month\": \"DEFAULT\"}, axis = 1, inplace = True)\n",
    "\n",
    "### We'll rename 'PAY_0' to 'PAY_1' to keep consistency in namings\n",
    "creditCard.rename({\"PAY_0\": \"PAY_1\"}, axis = 1, inplace = True)\n",
    "creditCard.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.Feature transformation\n",
    "\n",
    "According to the EDA, some continuous features exhibit non-normality. Below we apply the transformations that showed better results in the EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_1</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>DEFAULT</th>\n",
       "      <th>LIMIT_BAL_sqrt</th>\n",
       "      <th>BILL_AMT1_cbrt</th>\n",
       "      <th>PAY_AMT1_log</th>\n",
       "      <th>PAY_AMT2_log</th>\n",
       "      <th>PAY_AMT3_log</th>\n",
       "      <th>PAY_AMT4_log</th>\n",
       "      <th>PAY_AMT5_log</th>\n",
       "      <th>PAY_AMT6_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>3913.0</td>\n",
       "      <td>3102.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>141.421356</td>\n",
       "      <td>15.758079</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>6.535243</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>-6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>346.410162</td>\n",
       "      <td>13.893754</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>6.907756</td>\n",
       "      <td>6.907756</td>\n",
       "      <td>6.907756</td>\n",
       "      <td>-6.907755</td>\n",
       "      <td>7.600903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29239.0</td>\n",
       "      <td>14027.0</td>\n",
       "      <td>13559.0</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>30.807338</td>\n",
       "      <td>7.325150</td>\n",
       "      <td>7.313221</td>\n",
       "      <td>6.907756</td>\n",
       "      <td>6.907756</td>\n",
       "      <td>6.907756</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46990.0</td>\n",
       "      <td>48233.0</td>\n",
       "      <td>49291.0</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>223.606798</td>\n",
       "      <td>36.085701</td>\n",
       "      <td>7.600903</td>\n",
       "      <td>7.610358</td>\n",
       "      <td>7.090078</td>\n",
       "      <td>7.003066</td>\n",
       "      <td>6.974480</td>\n",
       "      <td>6.907756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8617.0</td>\n",
       "      <td>5670.0</td>\n",
       "      <td>35835.0</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "      <td>223.606798</td>\n",
       "      <td>20.501487</td>\n",
       "      <td>7.600903</td>\n",
       "      <td>10.510014</td>\n",
       "      <td>9.210340</td>\n",
       "      <td>9.104980</td>\n",
       "      <td>6.535243</td>\n",
       "      <td>6.520623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_1  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0     -2     -2     3913.0     3102.0      689.0        0.0        0.0   \n",
       "1      0      2     2682.0     1725.0     2682.0     3272.0     3455.0   \n",
       "2      0      0    29239.0    14027.0    13559.0    14331.0    14948.0   \n",
       "3      0      0    46990.0    48233.0    49291.0    28314.0    28959.0   \n",
       "4      0      0     8617.0     5670.0    35835.0    20940.0    19146.0   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0        0.0       0.0     689.0       0.0       0.0       0.0       0.0   \n",
       "1     3261.0       0.0    1000.0    1000.0    1000.0       0.0    2000.0   \n",
       "2    15549.0    1518.0    1500.0    1000.0    1000.0    1000.0    5000.0   \n",
       "3    29547.0    2000.0    2019.0    1200.0    1100.0    1069.0    1000.0   \n",
       "4    19131.0    2000.0   36681.0   10000.0    9000.0     689.0     679.0   \n",
       "\n",
       "   DEFAULT  LIMIT_BAL_sqrt  BILL_AMT1_cbrt  PAY_AMT1_log  PAY_AMT2_log  \\\n",
       "0        1      141.421356       15.758079     -6.907755      6.535243   \n",
       "1        1      346.410162       13.893754     -6.907755      6.907756   \n",
       "2        0      300.000000       30.807338      7.325150      7.313221   \n",
       "3        0      223.606798       36.085701      7.600903      7.610358   \n",
       "4        0      223.606798       20.501487      7.600903     10.510014   \n",
       "\n",
       "   PAY_AMT3_log  PAY_AMT4_log  PAY_AMT5_log  PAY_AMT6_log  \n",
       "0     -6.907755     -6.907755     -6.907755     -6.907755  \n",
       "1      6.907756      6.907756     -6.907755      7.600903  \n",
       "2      6.907756      6.907756      6.907756      8.517193  \n",
       "3      7.090078      7.003066      6.974480      6.907756  \n",
       "4      9.210340      9.104980      6.535243      6.520623  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditCard[\"LIMIT_BAL_sqrt\"] = np.sqrt(creditCard[\"LIMIT_BAL\"])\n",
    "creditCard[\"BILL_AMT1_cbrt\"] = np.cbrt(creditCard[\"BILL_AMT1\"])\n",
    "\n",
    "cols = [\"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]\n",
    "for col in cols:\n",
    "    name = col + \"_log\"\n",
    "    creditCard[name] = np.log(creditCard[col] + abs(creditCard[col].min()) + 0.001)\n",
    "\n",
    "creditCard.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Feature scaling\n",
    "\n",
    "We scale all continuous features using the Min-Max scaler. This scaler scales a feature in the range `[0, 1]` or in the range `[-1, 1]` if the feature has negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL_sqrt</th>\n",
       "      <th>BILL_AMT1_cbrt</th>\n",
       "      <th>PAY_AMT1_log</th>\n",
       "      <th>PAY_AMT2_log</th>\n",
       "      <th>PAY_AMT3_log</th>\n",
       "      <th>PAY_AMT4_log</th>\n",
       "      <th>PAY_AMT5_log</th>\n",
       "      <th>PAY_AMT6_log</th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.046024</td>\n",
       "      <td>0.459749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.632773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.273789</td>\n",
       "      <td>0.447621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650307</td>\n",
       "      <td>0.670217</td>\n",
       "      <td>0.682354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722332</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.557653</td>\n",
       "      <td>0.691318</td>\n",
       "      <td>0.669393</td>\n",
       "      <td>0.670217</td>\n",
       "      <td>0.682354</td>\n",
       "      <td>0.695253</td>\n",
       "      <td>0.767950</td>\n",
       "      <td>0.224138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.137341</td>\n",
       "      <td>0.591991</td>\n",
       "      <td>0.704712</td>\n",
       "      <td>0.683379</td>\n",
       "      <td>0.679062</td>\n",
       "      <td>0.687061</td>\n",
       "      <td>0.698611</td>\n",
       "      <td>0.687823</td>\n",
       "      <td>0.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137341</td>\n",
       "      <td>0.490608</td>\n",
       "      <td>0.704712</td>\n",
       "      <td>0.819868</td>\n",
       "      <td>0.781920</td>\n",
       "      <td>0.790876</td>\n",
       "      <td>0.676507</td>\n",
       "      <td>0.668549</td>\n",
       "      <td>0.620690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL_sqrt  BILL_AMT1_cbrt  PAY_AMT1_log  PAY_AMT2_log  PAY_AMT3_log  \\\n",
       "0        0.046024        0.459749      0.000000      0.632773      0.000000   \n",
       "1        0.273789        0.447621      0.000000      0.650307      0.670217   \n",
       "2        0.222222        0.557653      0.691318      0.669393      0.670217   \n",
       "3        0.137341        0.591991      0.704712      0.683379      0.679062   \n",
       "4        0.137341        0.490608      0.704712      0.819868      0.781920   \n",
       "\n",
       "   PAY_AMT4_log  PAY_AMT5_log  PAY_AMT6_log       AGE  \n",
       "0      0.000000      0.000000      0.000000  0.051724  \n",
       "1      0.682354      0.000000      0.722332  0.086207  \n",
       "2      0.682354      0.695253      0.767950  0.224138  \n",
       "3      0.687061      0.698611      0.687823  0.275862  \n",
       "4      0.790876      0.676507      0.668549  0.620690  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We extract the features that are going to be scaled\n",
    "cols = [\"LIMIT_BAL_sqrt\", \"BILL_AMT1_cbrt\", \"PAY_AMT1_log\", \"PAY_AMT2_log\", \"PAY_AMT3_log\", \"PAY_AMT4_log\", \\\n",
    "    \"PAY_AMT5_log\", \"PAY_AMT6_log\", \"AGE\"]\n",
    "creditCard_scaled = creditCard[cols].copy()\n",
    "\n",
    "### We apply the min-max scaler\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "creditCard_scaled[cols] = scaler.fit_transform(creditCard_scaled[cols])\n",
    "creditCard_scaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. One-hot encoding\n",
    "\n",
    "We dummify all categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEX_2</th>\n",
       "      <th>EDUCATION_1</th>\n",
       "      <th>EDUCATION_2</th>\n",
       "      <th>EDUCATION_3</th>\n",
       "      <th>EDUCATION_4</th>\n",
       "      <th>EDUCATION_5</th>\n",
       "      <th>EDUCATION_6</th>\n",
       "      <th>MARRIAGE_1</th>\n",
       "      <th>MARRIAGE_2</th>\n",
       "      <th>MARRIAGE_3</th>\n",
       "      <th>PAY_1_-1</th>\n",
       "      <th>PAY_1_0</th>\n",
       "      <th>PAY_1_1</th>\n",
       "      <th>PAY_1_2</th>\n",
       "      <th>PAY_1_3</th>\n",
       "      <th>PAY_1_4</th>\n",
       "      <th>PAY_1_5</th>\n",
       "      <th>PAY_1_6</th>\n",
       "      <th>PAY_1_7</th>\n",
       "      <th>PAY_1_8</th>\n",
       "      <th>PAY_2_-1</th>\n",
       "      <th>PAY_2_0</th>\n",
       "      <th>PAY_2_1</th>\n",
       "      <th>PAY_2_2</th>\n",
       "      <th>PAY_2_3</th>\n",
       "      <th>PAY_2_4</th>\n",
       "      <th>PAY_2_5</th>\n",
       "      <th>PAY_2_6</th>\n",
       "      <th>PAY_2_7</th>\n",
       "      <th>PAY_2_8</th>\n",
       "      <th>PAY_3_-1</th>\n",
       "      <th>PAY_3_0</th>\n",
       "      <th>PAY_3_1</th>\n",
       "      <th>PAY_3_2</th>\n",
       "      <th>PAY_3_3</th>\n",
       "      <th>PAY_3_4</th>\n",
       "      <th>PAY_3_5</th>\n",
       "      <th>PAY_3_6</th>\n",
       "      <th>PAY_3_7</th>\n",
       "      <th>PAY_3_8</th>\n",
       "      <th>PAY_4_-1</th>\n",
       "      <th>PAY_4_0</th>\n",
       "      <th>PAY_4_1</th>\n",
       "      <th>PAY_4_2</th>\n",
       "      <th>PAY_4_3</th>\n",
       "      <th>PAY_4_4</th>\n",
       "      <th>PAY_4_5</th>\n",
       "      <th>PAY_4_6</th>\n",
       "      <th>PAY_4_7</th>\n",
       "      <th>PAY_4_8</th>\n",
       "      <th>PAY_5_-1</th>\n",
       "      <th>PAY_5_0</th>\n",
       "      <th>PAY_5_2</th>\n",
       "      <th>PAY_5_3</th>\n",
       "      <th>PAY_5_4</th>\n",
       "      <th>PAY_5_5</th>\n",
       "      <th>PAY_5_6</th>\n",
       "      <th>PAY_5_7</th>\n",
       "      <th>PAY_5_8</th>\n",
       "      <th>PAY_6_-1</th>\n",
       "      <th>PAY_6_0</th>\n",
       "      <th>PAY_6_2</th>\n",
       "      <th>PAY_6_3</th>\n",
       "      <th>PAY_6_4</th>\n",
       "      <th>PAY_6_5</th>\n",
       "      <th>PAY_6_6</th>\n",
       "      <th>PAY_6_7</th>\n",
       "      <th>PAY_6_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEX_2  EDUCATION_1  EDUCATION_2  EDUCATION_3  EDUCATION_4  EDUCATION_5  \\\n",
       "0      1            0            1            0            0            0   \n",
       "1      1            0            1            0            0            0   \n",
       "2      1            0            1            0            0            0   \n",
       "3      1            0            1            0            0            0   \n",
       "4      0            0            1            0            0            0   \n",
       "\n",
       "   EDUCATION_6  MARRIAGE_1  MARRIAGE_2  MARRIAGE_3  PAY_1_-1  PAY_1_0  \\\n",
       "0            0           1           0           0         0        0   \n",
       "1            0           0           1           0         1        0   \n",
       "2            0           0           1           0         0        1   \n",
       "3            0           1           0           0         0        1   \n",
       "4            0           1           0           0         1        0   \n",
       "\n",
       "   PAY_1_1  PAY_1_2  PAY_1_3  PAY_1_4  PAY_1_5  PAY_1_6  PAY_1_7  PAY_1_8  \\\n",
       "0        0        1        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   PAY_2_-1  PAY_2_0  PAY_2_1  PAY_2_2  PAY_2_3  PAY_2_4  PAY_2_5  PAY_2_6  \\\n",
       "0         0        0        0        1        0        0        0        0   \n",
       "1         0        0        0        1        0        0        0        0   \n",
       "2         0        1        0        0        0        0        0        0   \n",
       "3         0        1        0        0        0        0        0        0   \n",
       "4         0        1        0        0        0        0        0        0   \n",
       "\n",
       "   PAY_2_7  PAY_2_8  PAY_3_-1  PAY_3_0  PAY_3_1  PAY_3_2  PAY_3_3  PAY_3_4  \\\n",
       "0        0        0         1        0        0        0        0        0   \n",
       "1        0        0         0        1        0        0        0        0   \n",
       "2        0        0         0        1        0        0        0        0   \n",
       "3        0        0         0        1        0        0        0        0   \n",
       "4        0        0         1        0        0        0        0        0   \n",
       "\n",
       "   PAY_3_5  PAY_3_6  PAY_3_7  PAY_3_8  PAY_4_-1  PAY_4_0  PAY_4_1  PAY_4_2  \\\n",
       "0        0        0        0        0         1        0        0        0   \n",
       "1        0        0        0        0         0        1        0        0   \n",
       "2        0        0        0        0         0        1        0        0   \n",
       "3        0        0        0        0         0        1        0        0   \n",
       "4        0        0        0        0         0        1        0        0   \n",
       "\n",
       "   PAY_4_3  PAY_4_4  PAY_4_5  PAY_4_6  PAY_4_7  PAY_4_8  PAY_5_-1  PAY_5_0  \\\n",
       "0        0        0        0        0        0        0         0        0   \n",
       "1        0        0        0        0        0        0         0        1   \n",
       "2        0        0        0        0        0        0         0        1   \n",
       "3        0        0        0        0        0        0         0        1   \n",
       "4        0        0        0        0        0        0         0        1   \n",
       "\n",
       "   PAY_5_2  PAY_5_3  PAY_5_4  PAY_5_5  PAY_5_6  PAY_5_7  PAY_5_8  PAY_6_-1  \\\n",
       "0        0        0        0        0        0        0        0         0   \n",
       "1        0        0        0        0        0        0        0         0   \n",
       "2        0        0        0        0        0        0        0         0   \n",
       "3        0        0        0        0        0        0        0         0   \n",
       "4        0        0        0        0        0        0        0         0   \n",
       "\n",
       "   PAY_6_0  PAY_6_2  PAY_6_3  PAY_6_4  PAY_6_5  PAY_6_6  PAY_6_7  PAY_6_8  \n",
       "0        0        0        0        0        0        0        0        0  \n",
       "1        0        1        0        0        0        0        0        0  \n",
       "2        1        0        0        0        0        0        0        0  \n",
       "3        1        0        0        0        0        0        0        0  \n",
       "4        1        0        0        0        0        0        0        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### For the sake of thoroughness, we convert the categorical features into actual categorical features \n",
    "cols = [\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"PAY_1\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"]\n",
    "creditCard[cols] = creditCard[cols].astype(\"category\")\n",
    "creditCard_dumm = creditCard[cols].copy()\n",
    "\n",
    "### We dummify all the categorical features\n",
    "creditCard_dumm = pd.get_dummies(creditCard_dumm, columns = cols, drop_first = True)\n",
    "creditCard_dumm.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Processed DataFrame\n",
    "\n",
    "We bring all transformations, scalings and dummies into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL_sqrt</th>\n",
       "      <th>BILL_AMT1_cbrt</th>\n",
       "      <th>PAY_AMT1_log</th>\n",
       "      <th>PAY_AMT2_log</th>\n",
       "      <th>PAY_AMT3_log</th>\n",
       "      <th>PAY_AMT4_log</th>\n",
       "      <th>PAY_AMT5_log</th>\n",
       "      <th>PAY_AMT6_log</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX_2</th>\n",
       "      <th>EDUCATION_1</th>\n",
       "      <th>EDUCATION_2</th>\n",
       "      <th>EDUCATION_3</th>\n",
       "      <th>EDUCATION_4</th>\n",
       "      <th>EDUCATION_5</th>\n",
       "      <th>EDUCATION_6</th>\n",
       "      <th>MARRIAGE_1</th>\n",
       "      <th>MARRIAGE_2</th>\n",
       "      <th>MARRIAGE_3</th>\n",
       "      <th>PAY_1_-1</th>\n",
       "      <th>PAY_1_0</th>\n",
       "      <th>PAY_1_1</th>\n",
       "      <th>PAY_1_2</th>\n",
       "      <th>PAY_1_3</th>\n",
       "      <th>PAY_1_4</th>\n",
       "      <th>PAY_1_5</th>\n",
       "      <th>PAY_1_6</th>\n",
       "      <th>PAY_1_7</th>\n",
       "      <th>PAY_1_8</th>\n",
       "      <th>PAY_2_-1</th>\n",
       "      <th>PAY_2_0</th>\n",
       "      <th>PAY_2_1</th>\n",
       "      <th>PAY_2_2</th>\n",
       "      <th>PAY_2_3</th>\n",
       "      <th>PAY_2_4</th>\n",
       "      <th>PAY_2_5</th>\n",
       "      <th>PAY_2_6</th>\n",
       "      <th>PAY_2_7</th>\n",
       "      <th>PAY_2_8</th>\n",
       "      <th>PAY_3_-1</th>\n",
       "      <th>PAY_3_0</th>\n",
       "      <th>PAY_3_1</th>\n",
       "      <th>PAY_3_2</th>\n",
       "      <th>PAY_3_3</th>\n",
       "      <th>PAY_3_4</th>\n",
       "      <th>PAY_3_5</th>\n",
       "      <th>PAY_3_6</th>\n",
       "      <th>PAY_3_7</th>\n",
       "      <th>PAY_3_8</th>\n",
       "      <th>PAY_4_-1</th>\n",
       "      <th>PAY_4_0</th>\n",
       "      <th>PAY_4_1</th>\n",
       "      <th>PAY_4_2</th>\n",
       "      <th>PAY_4_3</th>\n",
       "      <th>PAY_4_4</th>\n",
       "      <th>PAY_4_5</th>\n",
       "      <th>PAY_4_6</th>\n",
       "      <th>PAY_4_7</th>\n",
       "      <th>PAY_4_8</th>\n",
       "      <th>PAY_5_-1</th>\n",
       "      <th>PAY_5_0</th>\n",
       "      <th>PAY_5_2</th>\n",
       "      <th>PAY_5_3</th>\n",
       "      <th>PAY_5_4</th>\n",
       "      <th>PAY_5_5</th>\n",
       "      <th>PAY_5_6</th>\n",
       "      <th>PAY_5_7</th>\n",
       "      <th>PAY_5_8</th>\n",
       "      <th>PAY_6_-1</th>\n",
       "      <th>PAY_6_0</th>\n",
       "      <th>PAY_6_2</th>\n",
       "      <th>PAY_6_3</th>\n",
       "      <th>PAY_6_4</th>\n",
       "      <th>PAY_6_5</th>\n",
       "      <th>PAY_6_6</th>\n",
       "      <th>PAY_6_7</th>\n",
       "      <th>PAY_6_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.046024</td>\n",
       "      <td>0.459749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.632773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.273789</td>\n",
       "      <td>0.447621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650307</td>\n",
       "      <td>0.670217</td>\n",
       "      <td>0.682354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722332</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.557653</td>\n",
       "      <td>0.691318</td>\n",
       "      <td>0.669393</td>\n",
       "      <td>0.670217</td>\n",
       "      <td>0.682354</td>\n",
       "      <td>0.695253</td>\n",
       "      <td>0.767950</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.137341</td>\n",
       "      <td>0.591991</td>\n",
       "      <td>0.704712</td>\n",
       "      <td>0.683379</td>\n",
       "      <td>0.679062</td>\n",
       "      <td>0.687061</td>\n",
       "      <td>0.698611</td>\n",
       "      <td>0.687823</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137341</td>\n",
       "      <td>0.490608</td>\n",
       "      <td>0.704712</td>\n",
       "      <td>0.819868</td>\n",
       "      <td>0.781920</td>\n",
       "      <td>0.790876</td>\n",
       "      <td>0.676507</td>\n",
       "      <td>0.668549</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL_sqrt  BILL_AMT1_cbrt  PAY_AMT1_log  PAY_AMT2_log  PAY_AMT3_log  \\\n",
       "0        0.046024        0.459749      0.000000      0.632773      0.000000   \n",
       "1        0.273789        0.447621      0.000000      0.650307      0.670217   \n",
       "2        0.222222        0.557653      0.691318      0.669393      0.670217   \n",
       "3        0.137341        0.591991      0.704712      0.683379      0.679062   \n",
       "4        0.137341        0.490608      0.704712      0.819868      0.781920   \n",
       "\n",
       "   PAY_AMT4_log  PAY_AMT5_log  PAY_AMT6_log       AGE  SEX_2  EDUCATION_1  \\\n",
       "0      0.000000      0.000000      0.000000  0.051724      1            0   \n",
       "1      0.682354      0.000000      0.722332  0.086207      1            0   \n",
       "2      0.682354      0.695253      0.767950  0.224138      1            0   \n",
       "3      0.687061      0.698611      0.687823  0.275862      1            0   \n",
       "4      0.790876      0.676507      0.668549  0.620690      0            0   \n",
       "\n",
       "   EDUCATION_2  EDUCATION_3  EDUCATION_4  EDUCATION_5  EDUCATION_6  \\\n",
       "0            1            0            0            0            0   \n",
       "1            1            0            0            0            0   \n",
       "2            1            0            0            0            0   \n",
       "3            1            0            0            0            0   \n",
       "4            1            0            0            0            0   \n",
       "\n",
       "   MARRIAGE_1  MARRIAGE_2  MARRIAGE_3  PAY_1_-1  PAY_1_0  PAY_1_1  PAY_1_2  \\\n",
       "0           1           0           0         0        0        0        1   \n",
       "1           0           1           0         1        0        0        0   \n",
       "2           0           1           0         0        1        0        0   \n",
       "3           1           0           0         0        1        0        0   \n",
       "4           1           0           0         1        0        0        0   \n",
       "\n",
       "   PAY_1_3  PAY_1_4  PAY_1_5  PAY_1_6  PAY_1_7  PAY_1_8  PAY_2_-1  PAY_2_0  \\\n",
       "0        0        0        0        0        0        0         0        0   \n",
       "1        0        0        0        0        0        0         0        0   \n",
       "2        0        0        0        0        0        0         0        1   \n",
       "3        0        0        0        0        0        0         0        1   \n",
       "4        0        0        0        0        0        0         0        1   \n",
       "\n",
       "   PAY_2_1  PAY_2_2  PAY_2_3  PAY_2_4  PAY_2_5  PAY_2_6  PAY_2_7  PAY_2_8  \\\n",
       "0        0        1        0        0        0        0        0        0   \n",
       "1        0        1        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   PAY_3_-1  PAY_3_0  PAY_3_1  PAY_3_2  PAY_3_3  PAY_3_4  PAY_3_5  PAY_3_6  \\\n",
       "0         1        0        0        0        0        0        0        0   \n",
       "1         0        1        0        0        0        0        0        0   \n",
       "2         0        1        0        0        0        0        0        0   \n",
       "3         0        1        0        0        0        0        0        0   \n",
       "4         1        0        0        0        0        0        0        0   \n",
       "\n",
       "   PAY_3_7  PAY_3_8  PAY_4_-1  PAY_4_0  PAY_4_1  PAY_4_2  PAY_4_3  PAY_4_4  \\\n",
       "0        0        0         1        0        0        0        0        0   \n",
       "1        0        0         0        1        0        0        0        0   \n",
       "2        0        0         0        1        0        0        0        0   \n",
       "3        0        0         0        1        0        0        0        0   \n",
       "4        0        0         0        1        0        0        0        0   \n",
       "\n",
       "   PAY_4_5  PAY_4_6  PAY_4_7  PAY_4_8  PAY_5_-1  PAY_5_0  PAY_5_2  PAY_5_3  \\\n",
       "0        0        0        0        0         0        0        0        0   \n",
       "1        0        0        0        0         0        1        0        0   \n",
       "2        0        0        0        0         0        1        0        0   \n",
       "3        0        0        0        0         0        1        0        0   \n",
       "4        0        0        0        0         0        1        0        0   \n",
       "\n",
       "   PAY_5_4  PAY_5_5  PAY_5_6  PAY_5_7  PAY_5_8  PAY_6_-1  PAY_6_0  PAY_6_2  \\\n",
       "0        0        0        0        0        0         0        0        0   \n",
       "1        0        0        0        0        0         0        0        1   \n",
       "2        0        0        0        0        0         0        1        0   \n",
       "3        0        0        0        0        0         0        1        0   \n",
       "4        0        0        0        0        0         0        1        0   \n",
       "\n",
       "   PAY_6_3  PAY_6_4  PAY_6_5  PAY_6_6  PAY_6_7  PAY_6_8  \n",
       "0        0        0        0        0        0        0  \n",
       "1        0        0        0        0        0        0  \n",
       "2        0        0        0        0        0        0  \n",
       "3        0        0        0        0        0        0  \n",
       "4        0        0        0        0        0        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We concatenate the scaled features with the dummified features\n",
    "creditCard_ml = pd.concat([creditCard_scaled, creditCard_dumm], axis = 1)\n",
    "\n",
    "### We delete intermediate DataFrames\n",
    "del creditCard_scaled, creditCard_dumm\n",
    "\n",
    "creditCard_ml.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training, validation and test set splitting\n",
    "\n",
    "We split the dataset into training, validation and test sets. Comparing training and validation errors will allow us to assess whether a model is underfitting (has a high bias) or overfitting (has a high variance) the data.\n",
    "\n",
    "Underfitting means the model isn't doing well in the training set (the training error is high and the validation error is similar), while overfitting means the model isn't doing well in the validation set (the training error may be low but the validation error is much higher).\n",
    "\n",
    "Note that to assess whether a model is doing well in the training set (is the training error high?) we need a baseline level of performance. To establish this baseline level, we need to determine what's the error level we hope our model can get to. Common ways of establishing this baseline level are measuring how well humans do in the task, using the performance of a competing algorithm as the baseline level or using an informed guess based on prior experience.\n",
    "\n",
    "Comparing validation errors among several models will allow us to select a model. Finally, once a model is selected, its test error is taken as the model's generalization error.\n",
    "\n",
    "Note the validation set is often called cross-validation set. Cross-validation can also refer to the k-fold cross-validation process. I use the terms validation set and validation error to avoid confusion with the k-fold cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's split the dataset into training, validation and test sets keeping class proportions\n",
    "\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_\n",
    "x_train, x_, y_train, y_ = train_test_split(creditCard_ml, creditCard[\"DEFAULT\"], test_size = 0.40, \\\n",
    "    stratify = creditCard[\"DEFAULT\"], random_state = 1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for the validation set and the other for the test set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_, y_, test_size = 0.50, stratify = y_, random_state = 1)\n",
    "\n",
    "# Delete temporary variables\n",
    "del x_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set (input) is: (18000, 77)\n",
      "The shape of the training set (target) is: (18000,)\n",
      "\n",
      "The shape of the validation set (input) is: (6000, 77)\n",
      "The shape of the validation set (target) is: (6000,)\n",
      "\n",
      "The shape of the test set (input) is: (6000, 77)\n",
      "The shape of the test set (target) is: (6000,)\n"
     ]
    }
   ],
   "source": [
    "### Let's check the size of the 3 sets\n",
    "\n",
    "print(f\"The shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"The shape of the training set (target) is: {y_train.shape}\")\n",
    "print(f\"\\nThe shape of the validation set (input) is: {x_val.shape}\")\n",
    "print(f\"The shape of the validation set (target) is: {y_val.shape}\")\n",
    "print(f\"\\nThe shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"The shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING set\n",
      "Proportion of defaulters: 0.2212222222222222\n",
      "Proportion of non-defaulters 0.7787777777777778\n",
      "\n",
      "VALIDATION set\n",
      "Proportion of defaulters: 0.22116666666666668\n",
      "Proportion of non-defaulters 0.7788333333333334\n",
      "\n",
      "TEST set\n",
      "Proportion of defaulters: 0.22116666666666668\n",
      "Proportion of non-defaulters 0.7788333333333334\n"
     ]
    }
   ],
   "source": [
    "### Let's check whether the proportion of classes is similar among the 3 sets\n",
    "\n",
    "print(\"TRAINING set\")\n",
    "print(\"Proportion of defaulters: \" + str(len(y_train.loc[y_train == 1])/len(y_train)) + \"\\n\" + \\\n",
    "     \"Proportion of non-defaulters \" + str(len(y_train.loc[y_train == 0])/len(y_train)))\n",
    "print(\"\\nVALIDATION set\")\n",
    "print(\"Proportion of defaulters: \" + str(len(y_val.loc[y_val == 1])/len(y_val)) + \"\\n\" + \\\n",
    "     \"Proportion of non-defaulters \" + str(len(y_val.loc[y_val == 0])/len(y_val)))\n",
    "print(\"\\nTEST set\")\n",
    "print(\"Proportion of defaulters: \" + str(len(y_test.loc[y_test == 1])/len(y_test)) + \"\\n\" + \\\n",
    "     \"Proportion of non-defaulters \" + str(len(y_test.loc[y_test == 0])/len(y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Logistic regression\n",
    "\n",
    "Without a baseline performance level to assess whether the training set is performing well, we can only assess whether the model is overfitting by comparing the training and validation errors. Both errors are pretty similar (there's less than a 1% difference between them), which indicates the model isn't overfitting the data.\n",
    "\n",
    "Note we use verbosity for printing some log from the training process and a random state for ensuring reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200, random_state=1, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's train our logistic regression model\n",
    "\n",
    "# The default configuration doesn't work properly. We increased the iterations\n",
    "    # https://stackoverflow.com/questions/62658215/convergencewarning-lbfgs-failed-to-converge-status-1-stop-total-no-of-iter\n",
    "\n",
    "logisticRegr = LogisticRegression(max_iter = 200, verbose = 1, random_state = 1)\n",
    "logisticRegr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  0.8223888888888888\n",
      "MSE on training set:  0.1776111111111111\n",
      "1 - MSE:  0.8223888888888888\n"
     ]
    }
   ],
   "source": [
    "### Let's check the accuracy and error (MSE) of the training set\n",
    "\n",
    "y_pred = logisticRegr.predict(x_train)\n",
    "accuracy_training = logisticRegr.score(x_train, y_train) # metrics.accuracy_score(y_train, y_pred) delivers the same score\n",
    "mse_training = metrics.mean_squared_error(y_train, y_pred)\n",
    "print(\"Accuracy on training set: \", accuracy_training)\n",
    "print(\"MSE on training set: \", mse_training)\n",
    "print(\"1 - MSE: \", 1 - mse_training) # Just to check the relationship between accuracy and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.8168333333333333\n",
      "MSE on validation set:  0.18316666666666667\n"
     ]
    }
   ],
   "source": [
    "### Let's check the accuracy and error of the validation set\n",
    "\n",
    "y_pred = logisticRegr.predict(x_val)\n",
    "accuracy_val = logisticRegr.score(x_val, y_val)\n",
    "mse_val = metrics.mean_squared_error(y_val, y_pred)\n",
    "print(\"Accuracy on validation set: \", accuracy_val)\n",
    "print(\"MSE on validation set: \", mse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.8238333333333333\n",
      "MSE on test set:  0.17616666666666667\n"
     ]
    }
   ],
   "source": [
    "### Let's check the accuracy and error of the test set\n",
    "\n",
    "y_pred = logisticRegr.predict(x_test)\n",
    "accuracy_test = logisticRegr.score(x_test, y_test)\n",
    "mse_test = metrics.mean_squared_error(y_test, y_pred)\n",
    "print(\"Accuracy on test set: \", accuracy_test)\n",
    "print(\"MSE on test set: \", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Let's compute and plot the confusion matrix (on the validation set)\n",
    "\n",
    "# We compute the model predictions using the validation set\n",
    "y_pred = logisticRegr.predict(x_val)\n",
    "\n",
    "# We compute the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_val, y_pred).T\n",
    "\n",
    "# We plot the confusion matrix\n",
    "sns.heatmap(cm, annot = True, fmt = \".3f\", linewidths = .5, square = True, cmap = \"Blues_r\")\n",
    "plt.xlabel(\"Actual label\")\n",
    "plt.ylabel(\"Predicted label\")\n",
    "all_sample_title = \"Logistic regression\" + \"\\nAccuracy score: {:.4f}\".format(accuracy_val)\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.6596638655462185\n",
      "Recall:  0.3549359457422758\n",
      "F1 score:  0.4615384615384616\n",
      "ROC AUC:  0.6514675448805537\n"
     ]
    }
   ],
   "source": [
    "### Let's check the F1 score and AUC\n",
    "\n",
    "\"\"\"\n",
    "# We can manually compute the relevant scores from the confusion matrix\n",
    "precision = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "recall = cm[1,1] / (cm[1,1] + cm[0,1] )\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 score: \", f1)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Precision: \", metrics.precision_score(y_val, y_pred))\n",
    "print(\"Recall: \", metrics.recall_score(y_val, y_pred))\n",
    "print(\"F1 score: \", metrics.f1_score(y_val, y_pred))\n",
    "print(\"ROC AUC: \", metrics.roc_auc_score(y_val, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Extreme Gradient Boosting (XGBoost)\n",
    "\n",
    "Without a baseline performance level to assess whether the training set is performing well, we can only assess whether the model is overfitting by comparing the training and validation errors. With the `XGBClassifier()` default implementation, the validation error is somewhat higher than the training error (there's a 7% difference between them), which indicates the model is somewhat overfitting the data.\n",
    "\n",
    "In the next two sections we explore some basic strategies to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=1,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's train our XGBoost classifier\n",
    "\n",
    "xgbClass = XGBClassifier(verbosity = 1, random_state = 1)\n",
    "xgbClass.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  0.8895\n",
      "MSE on training set:  0.1105\n"
     ]
    }
   ],
   "source": [
    "### Let's check the accuracy and error of the training set\n",
    "\n",
    "y_pred = xgbClass.predict(x_train)\n",
    "accuracy_training = xgbClass.score(x_train, y_train)\n",
    "mse_training = metrics.mean_squared_error(y_train, y_pred)\n",
    "print(\"Accuracy on training set: \", accuracy_training)\n",
    "print(\"MSE on training set: \", mse_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on cross validation set:  0.8126666666666666\n",
      "MSE on cross validation set:  0.18733333333333332\n"
     ]
    }
   ],
   "source": [
    "### Let's check the accuracy and error of the validation set\n",
    "\n",
    "y_pred = xgbClass.predict(x_val)\n",
    "accuracy_val = xgbClass.score(x_val, y_val)\n",
    "mse_val = metrics.mean_squared_error(y_val, y_pred)\n",
    "print(\"Accuracy on cross validation set: \", accuracy_val)\n",
    "print(\"MSE on cross validation set: \", mse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.8175\n",
      "MSE on test set:  0.1825\n"
     ]
    }
   ],
   "source": [
    "### Let's check the accuracy and error of the test set\n",
    "\n",
    "y_pred = xgbClass.predict(x_test)\n",
    "accuracy_test = xgbClass.score(x_test, y_test)\n",
    "mse_test = metrics.mean_squared_error(y_test, y_pred)\n",
    "print(\"Accuracy on test set: \", accuracy_test)\n",
    "print(\"MSE on test set: \", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Let's compute and plot the confusion matrix (on the validation set)\n",
    "\n",
    "# We compute the model predictions using the validation set\n",
    "y_pred = xgbClass.predict(x_val)\n",
    "\n",
    "# We compute the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_val, y_pred).T\n",
    "\n",
    "# We plot the confusion matrix\n",
    "sns.heatmap(cm, annot = True, fmt = \".3f\", linewidths = .5, square = True, cmap = \"Blues_r\")\n",
    "plt.xlabel(\"Actual label\")\n",
    "plt.ylabel(\"Predicted label\")\n",
    "all_sample_title = \"Accuracy score: {0}\".format(accuracy_val)\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.636241610738255\n",
      "Recall:  0.3571966842501884\n",
      "F1 score:  0.4575289575289575\n",
      "ROC AUC:  0.6496019800450599\n"
     ]
    }
   ],
   "source": [
    "### Let's check the F1 score and AUC\n",
    "\n",
    "print(\"Precision: \", metrics.precision_score(y_val, y_pred))\n",
    "print(\"Recall: \", metrics.recall_score(y_val, y_pred))\n",
    "print(\"F1 score: \", metrics.f1_score(y_val, y_pred))\n",
    "print(\"ROC AUC: \", metrics.roc_auc_score(y_val, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Overfitting\n",
    "\n",
    "We can tune several parameters to reduce overfitting. Please visit [Parameters for Tree Booster](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster) and [This question from Stack Exchange](https://stats.stackexchange.com/questions/443259/how-to-avoid-overfitting-in-xgboost-model) for more info on what parameters can be tuned to reduce overfitting. We play with some common choices:\n",
    "\n",
    "##### subsample\n",
    "\n",
    "`xgbClass = XGBClassifier(subsample = 0.5, verbosity = 1, random_state = 1)`: overfitting doesn't improve.\n",
    "\n",
    "##### max_depth\n",
    "\n",
    "`xgbClass = XGBClassifier(max_depth = 4, verbosity = 1, random_state = 1)`: from 4 downwards overfitting improves.\n",
    "\n",
    "##### learning_rate\n",
    "\n",
    "`xgbClass = XGBClassifier(learning_rate = 0.1, verbosity = 1, random_state = 1)`: from 0.1 downwards overfitting improves.\n",
    "\n",
    "##### min_split_loss\n",
    "\n",
    "`xgbClass = XGBClassifier(min_split_loss = 3, verbosity = 1, random_state = 1)`: from 3 upwards overfitting improves.\n",
    "\n",
    "##### early_stopping_rounds\n",
    "\n",
    "    x_train_fit, x_train_eval, y_train_fit, y_train_eval = train_test_split(x_train, y_train, test_size = 0.2, \\\n",
    "        stratify = y_train, random_state = 1)   \n",
    "    xgbClass = XGBClassifier(verbosity = 1, random_state = 1)\n",
    "    xgbClass.fit(x_train_fit, y_train_fit, eval_set = [(x_train_eval, y_train_eval)], early_stopping_rounds = 10)\n",
    "    y_pred = xgbClass.predict(x_train_fit)\n",
    "    accuracy_training = xgbClass.score(x_train_fit, y_train_fit)\n",
    "    mse_training = metrics.mean_squared_error(y_train_fit, y_pred)\n",
    "    print(\"Accuracy on training set: \", accuracy_training)\n",
    "    print(\"MSE on training set: \", mse_training)\n",
    "\n",
    "This early stopping implementation somewhat improves overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Cross-validation\n",
    "\n",
    "We can also use cross-validation to reduce overfitting. The work below relies heavily on the process I followed while learning about how to make predictions after cross-validating a model. Most of this process is captured in a [Question I posted myself in Stack Overflow](https://stackoverflow.com/questions/75454460/does-it-make-sense-to-use-scikit-learn-cross-val-predict-to-imake-prediction).\n",
    "\n",
    "Note we use a XGBoost classifier with its default implementation for illustrative purposes. A more rigorous approach would require to systematically explore how the model performs with several parameter values using cross-validation. We briefly discuss this in Section **3.3. Summary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.26% (0.46%)\n"
     ]
    }
   ],
   "source": [
    "# We split the data on training and test sets. The training set is the one used in the cross-validation\n",
    "x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(creditCard_ml, creditCard[\"DEFAULT\"], test_size = 0.20, \\\n",
    "    stratify = creditCard[\"DEFAULT\"], random_state = 1)\n",
    "\n",
    "# We create the model\n",
    "xgbClassCV = XGBClassifier()\n",
    "\n",
    "# We define the folding method, using 5 folds for illustrative purposes\n",
    "kfold = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "# We run the cross-validation\n",
    "results_kfold = cross_val_score(xgbClassCV, x_train_cv, y_train_cv, cv = kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results_kfold.mean()*100, results_kfold.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on internal test sets:  0.812625\n",
      "MSE on internal test sets:  0.187375\n"
     ]
    }
   ],
   "source": [
    "### Let's check the accuracy and error of the internal test sets from the cross-validation process\n",
    "\n",
    "y_pred = cross_val_predict(xgbClassCV, x_train_cv, y_train_cv, cv = kfold)\n",
    "accuracy_test = metrics.accuracy_score(y_train_cv, y_pred)\n",
    "mse_test = metrics.mean_squared_error(y_train_cv, y_pred)\n",
    "print(\"Accuracy on internal test sets: \", accuracy_test)\n",
    "print(\"MSE on internal test sets: \", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Let's compute and plot the confusion matrix (on the internal test sets from the cross-validation process)\n",
    "\n",
    "# We compute the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_train_cv, y_pred).T\n",
    "\n",
    "# We plot the confusion matrix\n",
    "sns.heatmap(cm, annot = True, fmt = \".3f\", linewidths = .5, square = True, cmap = \"Blues_r\")\n",
    "plt.xlabel(\"Actual label\")\n",
    "plt.ylabel(\"Predicted label\")\n",
    "all_sample_title = \"XGBoost classifier\" + \"\\nAccuracy score: {:.4f}\".format(accuracy_test)\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.6389459274469541\n",
      "Recall:  0.3516669805989829\n",
      "F1 score:  0.4536508322196574\n",
      "ROC AUC:  0.6476113513021131\n"
     ]
    }
   ],
   "source": [
    "### Let's check the F1 score and AUC\n",
    "\n",
    "print(\"Precision: \", metrics.precision_score(y_train_cv, y_pred))\n",
    "print(\"Recall: \", metrics.recall_score(y_train_cv, y_pred))\n",
    "print(\"F1 score: \", metrics.f1_score(y_train_cv, y_pred))\n",
    "print(\"ROC AUC: \", metrics.roc_auc_score(y_train_cv, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, borrowed and adapted from [How to plot a confusion matrix from a k-fold cross-validation](https://towardsdatascience.com/how-to-plot-a-confusion-matrix-from-a-k-fold-cross-validation-b607317e9874), helps check whether `cross_val_predict()` works fine:\n",
    "\n",
    "    import copy as cp\n",
    "    from typing import Tuple\n",
    "    \n",
    "    def cross_val_predict_y(model, kfold : kfold, X : np.array, y : np.array) -> Tuple[np.array, np.array, np.array]:\n",
    "\n",
    "        model_ = cp.deepcopy(model)\n",
    "        \n",
    "        no_classes = len(np.unique(y))\n",
    "        \n",
    "        actual_classes = np.empty([0], dtype=int)\n",
    "        predicted_classes = np.empty([0], dtype=int)\n",
    "        predicted_proba = np.empty([0, no_classes]) \n",
    "\n",
    "        for train_ndx, test_ndx in kfold.split(X, y):\n",
    "\n",
    "            train_X, train_y, test_X, test_y = X[train_ndx], y[train_ndx], X[test_ndx], y[test_ndx]\n",
    "\n",
    "            actual_classes = np.append(actual_classes, test_y)\n",
    "\n",
    "            model_.fit(train_X, train_y)\n",
    "            predicted_classes = np.append(predicted_classes, model_.predict(test_X))\n",
    "\n",
    "            try:\n",
    "                predicted_proba = np.append(predicted_proba, model_.predict_proba(test_X), axis=0)\n",
    "            except:\n",
    "                predicted_proba = np.append(predicted_proba, np.zeros((len(test_X), no_classes), dtype=float), axis=0)\n",
    "\n",
    "        return actual_classes, predicted_classes, predicted_proba\n",
    "\n",
    "    actual_classes, predicted_classes, _ = cross_val_predict_y(xgbClassCV, kfold, \\\n",
    "        x_train_cv.to_numpy(), y_train_cv.to_numpy())\n",
    "\n",
    "We can then compute the accuracy, confusion matrix and F1 score using `actual_classes` and `predicted_classes`:\n",
    "\n",
    "    score = metrics.accuracy_score(actual_classes, predicted_classes)\n",
    "    cm = metrics.confusion_matrix(actual_classes, predicted_classes).T\n",
    "    f1_score = metrics.f1_score(actual_classes, predicted_classes)\n",
    "\n",
    "The accuracy, confusion matrix and F1 score should be the same as the ones obtained after running `cross_val_predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.8153333333333334\n",
      "MSE on test set:  0.18466666666666667\n"
     ]
    }
   ],
   "source": [
    "### Let's make predictions on the test set and compute the generalization error\n",
    "\n",
    "# We perform a grid search on the cross-validation models\n",
    "xgbClassGrid = GridSearchCV(estimator = xgbClassCV, param_grid = {}, cv = kfold)\n",
    "xgbClassGrid.fit(x_train_cv, y_train_cv)\n",
    "\n",
    "# We select the best model from the cross-validation to make predictions\n",
    "y_pred = xgbClassGrid.best_estimator_.predict(x_test_cv)\n",
    "\n",
    "# We check the accuracy and error of the test set\n",
    "accuracy_test = metrics.accuracy_score(y_test_cv, y_pred)\n",
    "mse_test = metrics.mean_squared_error(y_test_cv, y_pred)\n",
    "print(\"Accuracy on test set: \", accuracy_test)\n",
    "print(\"MSE on test set: \", mse_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Summary\n",
    "\n",
    "Comparing the results from the logistic regression to the results from the cross-validated XGBoost classifier, we can conclude both models perform virtually the same.\n",
    "\n",
    "| Model                     | Accuracy  | Precision | Recall    | F1 score  | ROC AUC   | Generalization error  |\n",
    "| ---                       | ---       | ---       | ---       | ---       | ---       | ---                   |\n",
    "| Logistic regression       | 0.8168    | 0.6597    | 0.3549    | 0.4615    | 0.6515    | 0.1762                |\n",
    "| Cross-validated XGBoost   | 0.8126    | 0.6389    | 0.3517    | 0.4537    | 0.6476    | 0.1847                |\n",
    "\n",
    "Overall, although the accuracies seem relatively high, the confusion matrices and their metrics tell us both models are rather poor. In particular, the recall score indicates both of them have a high rate of false negatives, i.e., the rate at which the models predict people as non-defaulters when in fact they did default is high. More details on how to interpret a confusion matrix and its metrics can be found in [What does your classification metric tell about your data?](https://towardsdatascience.com/what-does-your-classification-metric-tell-about-your-data-4a8f35408a8b). Moreover, the ROC AUCs indicate the models have a 65% chance of distinguishing between positive and negative classes, which doesn't sound really promising. More details on how to interpret the ROC AUC can be found in [Understanding AUC - ROC curve](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5).\n",
    "\n",
    "Note we only did some basic work to address overfitting. A more rigorous approach would require to perform a hyperpameter tuning using cross-validation. For example, this process would help us determine what are the best `max_depth` and `learning_rate` (among other hyperparameters) for our XGBoost classifier (e.g., see [A guide on XGBoost hyperparameters tuning](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook)). The process can also be run for the logistic regression, although the impact can be rather small (e.g., see [Do I need to tune logistic regression hyperparameters?](https://medium.com/codex/do-i-need-to-tune-logistic-regression-hyperparameters-1cb2b81fca69)). The blog entry [Cross-validation and hyperparameter tuning: how to optimise your machine learning model](https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d) is a great resource to understand how this process works. Moreover, it packs most of the estimation and evaluation work I do here in a single method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model comparison with cross-validation\n",
    "\n",
    "We'll take the analysis one step further and apply cross-validation to both models (with their default implementation) and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We split the data on training and test sets. The training set is the one used in the cross-validation\n",
    "x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(creditCard_ml, creditCard[\"DEFAULT\"], test_size = 0.20, \\\n",
    "    stratify = creditCard[\"DEFAULT\"], random_state = 1)\n",
    "\n",
    "### We create both models. For illustrative purposes, we'll work with their default implementation\n",
    "logisticRegrCV = LogisticRegression(verbose = 1, random_state = 1)\n",
    "xgbClassCV = XGBClassifier(verbosity = 1, random_state = 1)\n",
    "\n",
    "### We define the folding method, using 5 folds for illustrative purposes\n",
    "kfold = StratifiedKFold(n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "### We create lists to store performance metrics of all 5 folds\n",
    "accs = [] # Accuray scores\n",
    "precs = [] # Precision scores\n",
    "recalls = [] # Recall scores\n",
    "f1s = [] # F1 scores\n",
    "aucs = [] # ROC AUC scores\n",
    "\n",
    "### Accuracy\n",
    "# This retrieves the accuracy scores of the 5 folds\n",
    "accuracy = np.round(cross_val_score(logisticRegrCV, x_train_cv, y_train_cv, scoring = \"accuracy\", cv = kfold), 4)\n",
    "# We store them in a list (this is done to put the scores later in a DataFrame)\n",
    "accs.append(accuracy)\n",
    "# We compute the average score across the 5 folds\n",
    "accuracy_avg = round(accuracy.mean(), 4)\n",
    "\n",
    "### Precision\n",
    "precision = np.round(cross_val_score(logisticRegrCV, x_train_cv, y_train_cv, scoring = \"precision\", cv = kfold), 4)\n",
    "precs.append(precision)\n",
    "precision_avg = round(precision.mean(), 4)\n",
    "\n",
    "### Recall\n",
    "recall = np.round(cross_val_score(logisticRegrCV, x_train_cv, y_train_cv, scoring = \"recall\", cv = kfold), 4)\n",
    "recalls.append(recall)\n",
    "recall_avg = round(recall.mean(), 4)\n",
    "\n",
    "### F1\n",
    "f1 = np.round(cross_val_score(logisticRegrCV, x_train_cv, y_train_cv, scoring = \"f1\", cv = kfold), 4)\n",
    "f1s.append(f1)\n",
    "f1_avg = round(f1.mean(), 4)\n",
    "\n",
    "### ROC AUC\n",
    "auc = np.round(cross_val_score(logisticRegrCV, x_train_cv, y_train_cv, scoring = \"roc_auc\", cv = kfold), 4)\n",
    "aucs.append(auc)\n",
    "auc_avg = round(auc.mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "- Accuracy:  [0.8138 0.824  0.8179 0.8281 0.8202]  Avg:  0.8208\n",
      "- Precision:  [0.6473 0.69   0.6734 0.7054 0.6846]  Avg:  0.6801\n",
      "- Recall:  [0.3459 0.371  0.3437 0.3832 0.3475]  Avg:  0.3583\n",
      "- F1:  [0.4509 0.4825 0.4551 0.4966 0.461 ]  Avg:  0.4692\n",
      "- ROC AUC:  [0.7637 0.7752 0.7592 0.7906 0.7724]  Avg:  0.7722\n"
     ]
    }
   ],
   "source": [
    "### Let's check the logistic regression results\n",
    "\n",
    "print(\"Logistic regression\")\n",
    "print(\"- Accuracy: \", accuracy, \" Avg: \", accuracy_avg)\n",
    "print(\"- Precision: \", precision, \" Avg: \", precision_avg)\n",
    "print(\"- Recall: \", recall, \" Avg: \", recall_avg)\n",
    "print(\"- F1: \", f1, \" Avg: \", f1_avg)\n",
    "print(\"- ROC AUC: \", auc, \" Avg: \", auc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We repeat the process for the XGBoost classifier\n",
    "\n",
    "accs = []\n",
    "precs = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "aucs = []\n",
    "\n",
    "accuracy = np.round(cross_val_score(xgbClassCV, x_train_cv, y_train_cv, scoring = \"accuracy\", cv = kfold), 4)\n",
    "accs.append(accuracy)\n",
    "accuracy_avg = round(accuracy.mean(), 4)\n",
    "precision = np.round(cross_val_score(xgbClassCV, x_train_cv, y_train_cv, scoring = \"precision\", cv = kfold), 4)\n",
    "precs.append(precision)\n",
    "precision_avg = round(precision.mean(), 4)\n",
    "recall = np.round(cross_val_score(xgbClassCV, x_train_cv, y_train_cv, scoring = \"recall\", cv = kfold), 4)\n",
    "recalls.append(recall)\n",
    "recall_avg = round(recall.mean(), 4)\n",
    "f1 = np.round(cross_val_score(xgbClassCV, x_train_cv, y_train_cv, scoring = \"f1\", cv = kfold), 4)\n",
    "f1s.append(f1)\n",
    "f1_avg = round(f1.mean(), 4)\n",
    "auc = np.round(cross_val_score(xgbClassCV, x_train_cv, y_train_cv, scoring = \"roc_auc\", cv = kfold), 4)\n",
    "aucs.append(auc)\n",
    "auc_avg = round(auc.mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost classifier\n",
      "- Accuracy:  [0.81   0.8112 0.8073 0.8208 0.8138]  Avg:  0.8126\n",
      "- Precision:  [0.6305 0.6304 0.6187 0.6706 0.6438]  Avg:  0.6388\n",
      "- Recall:  [0.3393 0.355  0.3362 0.3738 0.354 ]  Avg:  0.3517\n",
      "- F1:  [0.4412 0.4542 0.4356 0.48   0.4569]  Avg:  0.4536\n",
      "- ROC AUC:  [0.7529 0.7646 0.7433 0.7794 0.7626]  Avg:  0.7606\n"
     ]
    }
   ],
   "source": [
    "### Let's check the XGBoosst classifier results\n",
    "\n",
    "print(\"XGBoost classifier\")\n",
    "print(\"- Accuracy: \", accuracy, \" Avg: \", accuracy_avg)\n",
    "print(\"- Precision: \", precision, \" Avg: \", precision_avg)\n",
    "print(\"- Recall: \", recall, \" Avg: \", recall_avg)\n",
    "print(\"- F1: \", f1, \" Avg: \", f1_avg)\n",
    "print(\"- ROC AUC: \", auc, \" Avg: \", auc_avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, borrowed and adapted from [Cross-validation and hyperparameter tuning: how to optimise your machine learning model](https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d), encapsulates the process above in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_comparison(models, names, X, y, cv):\n",
    "    \"\"\"\n",
    "    This function performs a k-fold cross-validation for several models.\n",
    "\n",
    "    Args:\n",
    "        models: list of models to be cross-validated.\n",
    "        names: list with model names.\n",
    "        X: training set used in the cross-validation.\n",
    "        y: target classes used in the cross-validation.\n",
    "        cv: the number of folds or method. \n",
    "\n",
    "    Returns:\n",
    "        cv_scores: DataFrame with the average scores of the cross-validation for each model.\n",
    "        accs: list with the accuracy of each fold for each model.\n",
    "        precs: list with the precision of each fold for each model.\n",
    "        recalls: list with the recall of each fold for each model.\n",
    "        f1s: list with the F1 score of each fold for each model.\n",
    "        aucs: list with the ROC AUC of each fold for each model.\n",
    "    \"\"\"\n",
    "    \n",
    "    cv_scores = pd.DataFrame()\n",
    "    accs = []\n",
    "    precs = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    \n",
    "    # Loop through the models, run the CV, add the average scores to the DataFrame and all the scores to their\n",
    "    # respective lists\n",
    "    for (model, name) in zip(models, names):\n",
    "        \n",
    "        accuracy = np.round(cross_val_score(model, X, y, scoring = \"accuracy\", cv = cv), 4)\n",
    "        accs.append(accuracy)\n",
    "        accuracy_avg = round(accuracy.mean(), 4)\n",
    "        precision = np.round(cross_val_score(model, X, y, scoring = \"precision\", cv = cv), 4)\n",
    "        precs.append(precision)\n",
    "        precision_avg = round(precision.mean(), 4)\n",
    "        recall = np.round(cross_val_score(model, X, y, scoring = \"recall\", cv = cv), 4)\n",
    "        recalls.append(recall)\n",
    "        recall_avg = round(recall.mean(), 4)\n",
    "        f1 = np.round(cross_val_score(model, X, y, scoring = \"f1\", cv = cv), 4)\n",
    "        f1s.append(f1)\n",
    "        f1_avg = round(f1.mean(), 4)\n",
    "        auc = np.round(cross_val_score(model, X, y, scoring = \"roc_auc\", cv = cv), 4)\n",
    "        aucs.append(auc)\n",
    "        auc_avg = round(auc.mean(), 4)\n",
    "        \n",
    "        # The model names are used as the feature names\n",
    "        cv_scores[str(name)] = [accuracy_avg, precision_avg, recall_avg, f1_avg, auc_avg]\n",
    "    \n",
    "    # The index of the DataFrame corresponds to the names of the performance metrics\n",
    "    cv_scores.index = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 score\", \"ROC AUC\"]\n",
    "    \n",
    "    return cv_scores, accs, precs, recalls, f1s, aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/jacasta2/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "### We create both models. For illustrative purposes, we'll work with their default implementation\n",
    "logisticRegrCV = LogisticRegression(random_state = 1)\n",
    "xgbClassCV = XGBClassifier(random_state = 1)\n",
    "\n",
    "### We define the folding method, using 5 folds for illustrative purposes\n",
    "kfold = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "### Put the models in a list\n",
    "models = [logisticRegrCV, xgbClassCV]\n",
    "names = [\"Logistic Regression\", \"XGBoost classifier\"]\n",
    "\n",
    "### Run the cross-validation comparison\n",
    "scores, accs, precs, recalls, f1s, aucs = cv_comparison(models, names, x_train_cv, y_train_cv, kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>XGBoost classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.8208</td>\n",
       "      <td>0.8126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.6801</td>\n",
       "      <td>0.6388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.3583</td>\n",
       "      <td>0.3517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score</th>\n",
       "      <td>0.4692</td>\n",
       "      <td>0.4536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.7722</td>\n",
       "      <td>0.7606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Logistic Regression  XGBoost classifier\n",
       "Accuracy                0.8208              0.8126\n",
       "Precision               0.6801              0.6388\n",
       "Recall                  0.3583              0.3517\n",
       "F1 score                0.4692              0.4536\n",
       "ROC AUC                 0.7722              0.7606"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's check the results\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can also check a given metric across each fold\n",
    "\n",
    "def foldMetrics(metricList, comparisonDf):\n",
    "    \"\"\"\n",
    "    This function returns a DataFrame with the value of a performance metric in each fold from a previously performed\n",
    "        cross-validation.\n",
    "\n",
    "    Args:\n",
    "        metricList: list with the values of the performance metric in all folds.\n",
    "        comparisonDf: DataFrame used to create the index of the returned DataFrame. The index corresponds to the\n",
    "            model names.\n",
    "        \n",
    "    Returns:\n",
    "        metricDf: DataFrame with the values of a performance metric for each fold of a cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    metricDf = pd.DataFrame(metricList, index = comparisonDf.columns, columns = [\"1st Fold\", \"2nd Fold\", \\\n",
    "        \"3rd fold\", \"4th fold\", \"5th fold\"])\n",
    "    metricDf[\"Average\"] = np.round(metricDf.mean(axis = 1), 4)\n",
    "\n",
    "    return metricDf                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Fold</th>\n",
       "      <th>2nd Fold</th>\n",
       "      <th>3rd fold</th>\n",
       "      <th>4th fold</th>\n",
       "      <th>5th fold</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.8138</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8179</td>\n",
       "      <td>0.8281</td>\n",
       "      <td>0.8202</td>\n",
       "      <td>0.8208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost classifier</th>\n",
       "      <td>0.8100</td>\n",
       "      <td>0.8112</td>\n",
       "      <td>0.8073</td>\n",
       "      <td>0.8208</td>\n",
       "      <td>0.8138</td>\n",
       "      <td>0.8126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1st Fold  2nd Fold  3rd fold  4th fold  5th fold  Average\n",
       "Logistic Regression    0.8138    0.8240    0.8179    0.8281    0.8202   0.8208\n",
       "XGBoost classifier     0.8100    0.8112    0.8073    0.8208    0.8138   0.8126"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's check the accuracies\n",
    "\n",
    "foldMetrics(accs, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Fold</th>\n",
       "      <th>2nd Fold</th>\n",
       "      <th>3rd fold</th>\n",
       "      <th>4th fold</th>\n",
       "      <th>5th fold</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.6473</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>0.6734</td>\n",
       "      <td>0.7054</td>\n",
       "      <td>0.6846</td>\n",
       "      <td>0.6801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost classifier</th>\n",
       "      <td>0.6305</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>0.6187</td>\n",
       "      <td>0.6706</td>\n",
       "      <td>0.6438</td>\n",
       "      <td>0.6388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1st Fold  2nd Fold  3rd fold  4th fold  5th fold  Average\n",
       "Logistic Regression    0.6473    0.6900    0.6734    0.7054    0.6846   0.6801\n",
       "XGBoost classifier     0.6305    0.6304    0.6187    0.6706    0.6438   0.6388"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's check the precisions\n",
    "\n",
    "foldMetrics(precs, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Fold</th>\n",
       "      <th>2nd Fold</th>\n",
       "      <th>3rd fold</th>\n",
       "      <th>4th fold</th>\n",
       "      <th>5th fold</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.3459</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.3437</td>\n",
       "      <td>0.3832</td>\n",
       "      <td>0.3475</td>\n",
       "      <td>0.3583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost classifier</th>\n",
       "      <td>0.3393</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.3362</td>\n",
       "      <td>0.3738</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>0.3517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1st Fold  2nd Fold  3rd fold  4th fold  5th fold  Average\n",
       "Logistic Regression    0.3459     0.371    0.3437    0.3832    0.3475   0.3583\n",
       "XGBoost classifier     0.3393     0.355    0.3362    0.3738    0.3540   0.3517"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's check the recalls\n",
    "\n",
    "foldMetrics(recalls, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Fold</th>\n",
       "      <th>2nd Fold</th>\n",
       "      <th>3rd fold</th>\n",
       "      <th>4th fold</th>\n",
       "      <th>5th fold</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.4509</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.4551</td>\n",
       "      <td>0.4966</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>0.4692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost classifier</th>\n",
       "      <td>0.4412</td>\n",
       "      <td>0.4542</td>\n",
       "      <td>0.4356</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4569</td>\n",
       "      <td>0.4536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1st Fold  2nd Fold  3rd fold  4th fold  5th fold  Average\n",
       "Logistic Regression    0.4509    0.4825    0.4551    0.4966    0.4610   0.4692\n",
       "XGBoost classifier     0.4412    0.4542    0.4356    0.4800    0.4569   0.4536"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's check the F1 scores\n",
    "\n",
    "foldMetrics(f1s, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Fold</th>\n",
       "      <th>2nd Fold</th>\n",
       "      <th>3rd fold</th>\n",
       "      <th>4th fold</th>\n",
       "      <th>5th fold</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.7637</td>\n",
       "      <td>0.7752</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.7724</td>\n",
       "      <td>0.7722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost classifier</th>\n",
       "      <td>0.7529</td>\n",
       "      <td>0.7646</td>\n",
       "      <td>0.7433</td>\n",
       "      <td>0.7794</td>\n",
       "      <td>0.7626</td>\n",
       "      <td>0.7606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1st Fold  2nd Fold  3rd fold  4th fold  5th fold  Average\n",
       "Logistic Regression    0.7637    0.7752    0.7592    0.7906    0.7724   0.7722\n",
       "XGBoost classifier     0.7529    0.7646    0.7433    0.7794    0.7626   0.7606"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's check the ROC AUCs\n",
    "\n",
    "foldMetrics(aucs, scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Overall, both models perform in a similar way and rather poorly. The confusion matrix metrics tell us both models are rather poor. In particular, the Recall score indicates both of them have a high rate of false negatives. Perhaps the only important difference is in precision, where the logistic regression clearly outperforms the XGBoost classifier. Checking the DataFrames of each performance metric allows to see in more detail how the metrics behaved during the cross-validation. This shows the poor performance is consistent across folds for both models and how the logistic regression consistently outperforms the XGBoost classifier in precision across folds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter tuning\n",
    "\n",
    "The final exercise in this model selection process we've been running is to fine-tune the hyperparameters of the models to find a good set of them for each model and make a final model comparison. To achieve this, we'll perform a hyperparameter tuning using cross-validation for each model. This is a more systematic approach than the manual iterations we ran in **Section 3.2.1** for the XGBoost classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We split the data on training and test sets. The training set is the one used in the cross-validation\n",
    "x_train_ht, x_test_ht, y_train_ht, y_test_ht = train_test_split(creditCard_ml, creditCard[\"DEFAULT\"], test_size = 0.20, \\\n",
    "    stratify = creditCard[\"DEFAULT\"], random_state = 1)\n",
    "\n",
    "### We define the folding method, using 5 folds for illustrative purposes\n",
    "kfold = StratifiedKFold(n_splits = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Logistic regression\n",
    "\n",
    "While the gains from a hyperparameter tuning of a logistic regression are overall rather small (e.g., see [Do I need to tune logistic regression hyperparameters?](https://medium.com/codex/do-i-need-to-tune-logistic-regression-hyperparameters-1cb2b81fca69)), we'll still perform this process for the sake of completeness. In [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) we can check the parameters we can fine-tune. Note we would need to check what combinations of the `solver` and `penalty` parameters can be used in the process. Since we'll try not so many parameter combinations, we can use a grid search, which will search the best parameters testing all possible parameter combinations (see [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for details about the cross-validation using grid search).\n",
    "\n",
    "[This question from Stack Overflow](https://stackoverflow.com/questions/67513075/what-is-c-parameter-in-sklearn-logistic-regression) and [Tuning parameters for logistic regression](https://www.kaggle.com/code/joparga3/2-tuning-parameters-for-logistic-regression) are additional sources I reviewed to perform the hyperparameter tuning of the logistic regression. The latter has code to plot validation curves, which are pretty useful to visualize the tradeoff between training and validation performance as we change the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We specify the hyperparameters and their different levels\n",
    "\n",
    "# The solver used in the optimization problem\n",
    "lgSolver = [\"lbfgs\", \"newton-cg\", \"sag\"] # 3 levels\n",
    "\n",
    "# The penalty used by the solver\n",
    "lgPenalty = [\"none\", \"l2\"] # 2 levels\n",
    "\n",
    "# The inverse of the regularization strength. Smaller values specify a stronger regularization\n",
    "lgC = [0.001, 0.01, 0.1, 1, 10] # 5 levels\n",
    "\n",
    "# We create the grid for the search. Note we'll test 3*2*5 = 30 parameter combinations\n",
    "lgGrid = {\"solver\": lgSolver,\n",
    "          \"penalty\": lgPenalty,\n",
    "          \"C\": lgC,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10],\n",
       "                         'penalty': ['none', 'l2'],\n",
       "                         'solver': ['lbfgs', 'newton-cg', 'sag']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We create the logistic regression\n",
    "logisticRegrHT = LogisticRegression()\n",
    "\n",
    "### We create the grid search\n",
    "logisticRegrGrid = GridSearchCV(estimator = logisticRegrHT, param_grid = lgGrid, cv = kfold, n_jobs = -1, \\\n",
    "    verbose = 3)\n",
    "\n",
    "### We run the grid search\n",
    "logisticRegrGrid.fit(x_train_ht, y_train_ht)\n",
    "    # The cell ran in 3m 29.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters\n",
      "{'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "\n",
      "Best accuracy score\n",
      "0.8211666666666668\n",
      "\n",
      "Mean accuracy score from the test set for each parameter combination\n",
      "[0.82116667 0.821      0.82108333 0.79708333 0.79708333 0.79708333\n",
      " 0.82116667 0.821      0.82108333 0.814375   0.814375   0.814375\n",
      " 0.82116667 0.821      0.82108333 0.82083333 0.82079167 0.82079167\n",
      " 0.82116667 0.821      0.82108333 0.82079167 0.82079167 0.82079167\n",
      " 0.82116667 0.821      0.82108333 0.82108333 0.82116667 0.82116667]\n",
      "\n",
      "Overall mean accuracy score from the test sets across all parameter combinations\n",
      "0.8179611111111112\n"
     ]
    }
   ],
   "source": [
    "### Let's check some of the results from the grid search process\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters\")\n",
    "print(logisticRegrGrid.best_params_)\n",
    "\n",
    "# Best score (the grid search computes accuracy by default, but we can specify a different metric)\n",
    "print(\"\\nBest accuracy score\")\n",
    "print(logisticRegrGrid.best_score_)\n",
    "\n",
    "# Mean score from the test set for each parameter combination (across the 5 folds)\n",
    "print(\"\\nMean accuracy score from the test set for each parameter combination\")\n",
    "print(logisticRegrGrid.cv_results_.get(\"mean_test_score\"))\n",
    "\n",
    "# Overall mean score from the test sets across all parameter combinations\n",
    "print(\"\\nOverall mean accuracy score from the test sets across all parameter combinations\")\n",
    "print(np.mean(logisticRegrGrid.cv_results_.get(\"mean_test_score\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. XGBoost classifier\n",
    "\n",
    "In this case, we'll fine-tune more hyperparameters, some of them with up to 10 levels or values. In [XGBoost parameters](https://xgboost.readthedocs.io/en/stable/parameter.html) we can check the parameters we can fine-tune.\n",
    "\n",
    "Since we'll try a large number of parameter combinations, a grid search won't be efficient since it'll consume a high computation power. Instead, we'll use a randomized grid search. Details about the cross-validation using a randomized grid search can be found in [sklearn.model_selection.RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). Moreover, [Hyper parameter tuning with randomised grid search](https://towardsdatascience.com/hyper-parameter-tuning-with-randomised-grid-search-54f865d27926) provides details about the efficiency of this approach and [Cross-validation and hyperparameter tuning: How to optimise your machine learning model](https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d) provides two examples of its application: with a random forest regressor and a XGBoost regressor.\n",
    "\n",
    "I reviewed some additional sources to perform the hyperparameter tuning of the XGBoost classifier (the last two also provide code to plot validation curves):\n",
    "\n",
    "- [A guide on XGBoost hyperparameters tuning](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook)\n",
    "- [Tuning XGBoost models](https://dwbi1.wordpress.com/2021/11/22/tuning-xgboost-models/)\n",
    "- [Complete guide to parameter tuning in XGBoost with codes in Python](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "- [Hyperparameter tuning in XGBoost](https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f)\n",
    "- [How to tune the number and size of decision trees with XGBoost in Python](https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/)\n",
    "- [Hyperparameter tuning for hyperaccurate XGBoost model](https://medium.com/broadhorizon-cmotions/hyperparameter-tuning-for-hyperaccurate-xgboost-model-d6e6b8650a11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We specify the hyperparameters and their different levels\n",
    "\n",
    "# Number of trees to be used\n",
    "xgb_n_estimators = [int(x) for x in np.linspace(50, 500, 10)] # 10 levels\n",
    "\n",
    "# Maximum number of levels in a tree\n",
    "xgb_max_depth = [int(x) for x in np.arange(2, 9)] # 7 levels\n",
    "\n",
    "# Learning rate\n",
    "xgb_eta = [0.01, 0.03, 0.1, 0.3] # 4 levels\n",
    "\n",
    "# Minimum number of observations (rows) needed in each node\n",
    "xgb_min_child_weight = [int(x) for x in np.arange(1, 11)] # 10 levels\n",
    "\n",
    "# Fraction of rows to sample when constructing each tree\n",
    "xgb_subsample = [x for x in np.linspace(0.5, 1, 6)] # 6 levels\n",
    "\n",
    "# Fraction of features (columns) to sample when constructing each tree\n",
    "xgb_colsample_bytree = [x for x in np.linspace(0.5, 1, 6)] # 6 levels\n",
    "\n",
    "# Minimum loss reduction required to make further partitions\n",
    "xgb_gamma = [x for x in np.arange(0, 11, 2)] # 6 levels\n",
    "\n",
    "# We create the grid for the search. Note we would test 10*7*4*10*6*6*6 = 604,800 parameter combinations!\n",
    "xgb_grid = {\"n_estimators\": xgb_n_estimators,\n",
    "            \"max_depth\": xgb_max_depth,\n",
    "            \"eta\": xgb_eta,\n",
    "            \"min_child_weight\": xgb_min_child_weight,\n",
    "            \"subsample\": xgb_subsample,\n",
    "            \"colsample_bytree\": xgb_colsample_bytree,\n",
    "            \"gamma\": xgb_gamma,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, gamma=None,\n",
       "                                           gpu_id=None, grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           int...\n",
       "                                           reg_alpha=None, reg_lambda=None, ...),\n",
       "                   n_iter=60, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.5, 0.6, 0.7, 0.8,\n",
       "                                                             0.9, 1.0],\n",
       "                                        'eta': [0.01, 0.03, 0.1, 0.3],\n",
       "                                        'gamma': [0, 2, 4, 6, 8, 10],\n",
       "                                        'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
       "                                        'min_child_weight': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10],\n",
       "                                        'n_estimators': [50, 100, 150, 200, 250,\n",
       "                                                         300, 350, 400, 450,\n",
       "                                                         500],\n",
       "                                        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9,\n",
       "                                                      1.0]},\n",
       "                   random_state=1, verbose=2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We create the XGBoost classifier\n",
    "xgbClassHT = XGBClassifier()\n",
    "\n",
    "### We create the randomized grid search\n",
    "xgbClassRand = RandomizedSearchCV(estimator = xgbClassHT, param_distributions = xgb_grid, n_iter = 60, \\\n",
    "    cv = kfold, n_jobs = -1, verbose = 2, random_state = 1)\n",
    "\n",
    "### We run the randomized grid search\n",
    "xgbClassRand.fit(x_train_ht, y_train_ht)\n",
    "    # The cell ran in 20m 12.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters\n",
      "{'subsample': 0.7, 'n_estimators': 200, 'min_child_weight': 6, 'max_depth': 6, 'gamma': 0, 'eta': 0.03, 'colsample_bytree': 0.7}\n",
      "\n",
      "Best accuracy score\n",
      "0.8212916666666669\n",
      "\n",
      "Mean accuracy score from the test set for each parameter combination\n",
      "[0.82033333 0.81979167 0.820625   0.79791667 0.81991667 0.82020833\n",
      " 0.82070833 0.81416667 0.82129167 0.81908333 0.82016667 0.81070833\n",
      " 0.81954167 0.81891667 0.81945833 0.81908333 0.82004167 0.81091667\n",
      " 0.818625   0.81558333 0.79833333 0.82091667 0.817875   0.81870833\n",
      " 0.81975    0.81416667 0.821      0.81275    0.81925    0.82045833\n",
      " 0.81945833 0.820625   0.818375   0.819      0.82045833 0.81391667\n",
      " 0.81954167 0.82041667 0.817125   0.8185     0.820625   0.81916667\n",
      " 0.820875   0.8195     0.81895833 0.82095833 0.82104167 0.81141667\n",
      " 0.81958333 0.79579167 0.819375   0.81741667 0.81854167 0.81975\n",
      " 0.81979167 0.82008333 0.82045833 0.82033333 0.79883333 0.816625  ]\n",
      "\n",
      "Overall mean accuracy score from the test sets across all parameter combinations\n",
      "0.8172805555555556\n"
     ]
    }
   ],
   "source": [
    "### Let's check some of the results from the randomized grid search process\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters\")\n",
    "print(xgbClassRand.best_params_)\n",
    "\n",
    "# Best score (the search computes accuracy by default, but we can specify a different metric)\n",
    "print(\"\\nBest accuracy score\")\n",
    "print(xgbClassRand.best_score_)\n",
    "\n",
    "# Mean score from the test set for each parameter combination (across the 5 folds)\n",
    "print(\"\\nMean accuracy score from the test set for each parameter combination\")\n",
    "print(xgbClassRand.cv_results_.get(\"mean_test_score\"))\n",
    "\n",
    "# Overall mean score from the test sets across all parameter combinations\n",
    "print(\"\\nOverall mean accuracy score from the test sets across all parameter combinations\")\n",
    "print(np.mean(xgbClassRand.cv_results_.get(\"mean_test_score\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Final validation\n",
    "\n",
    "Finally, we'll train and validate each fine-tuned model and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
       "              early_stopping_rounds=None, enable_categorical=False, eta=0.03,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.0299999993, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=6,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=200,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, ...)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's train the models\n",
    "\n",
    "# We create the logistic regression\n",
    "logisticRegrFT = LogisticRegression(solver = \"newton-cg\",\n",
    "    penalty = \"l2\",\n",
    "    C = 10)\n",
    "\n",
    "# We create the XGBoost classifier\n",
    "xgbClassFT = XGBClassifier(n_estimators = 200,\n",
    "    max_depth = 6,\n",
    "    eta = 0.03,\n",
    "    min_child_weight = 6,\n",
    "    subsample = 0.7,\n",
    "    colsample_bytree = 0.7,\n",
    "    gamma = 0)\n",
    "\n",
    "# We train the models\n",
    "logisticRegrFT.fit(x_train_ht, y_train_ht)\n",
    "xgbClassFT.fit(x_train_ht, y_train_ht)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, borrowed and adapted from [Cross-validation and hyperparameter tuning: how to optimise your machine learning model](https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d), encapsulates the validation process in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comparison(models, names, X, y):\n",
    "    \"\"\"\n",
    "    This function validates several models.\n",
    "\n",
    "    Args:\n",
    "        models: list of models to be validated.\n",
    "        names: list with model names.\n",
    "        X: validation set.\n",
    "        y: target classes used in the validation.\n",
    "        \n",
    "    Returns:\n",
    "        cv_scores: DataFrame with the scores of the prediction for each model.\n",
    "    \"\"\"\n",
    "    \n",
    "    cv_scores = pd.DataFrame()\n",
    "    \n",
    "    # Loop through the models, make predictions and add the scores to the DataFrame\n",
    "    for (model, name) in zip(models, names):\n",
    "        \n",
    "        predictions = model.predict(X)\n",
    "        accuracy = metrics.accuracy_score(y, predictions)\n",
    "        precision = metrics.precision_score(y, predictions)\n",
    "        recall = metrics.recall_score(y, predictions)\n",
    "        f1 = metrics.f1_score(y, predictions)\n",
    "        auc = metrics.roc_auc_score(y, predictions)\n",
    "        cm = metrics.confusion_matrix(y, predictions).T\n",
    "\n",
    "        # The model names are used as the feature names\n",
    "        cv_scores[str(name)] = [accuracy, precision, recall, f1, auc, cm]\n",
    "    \n",
    "    # The index of the DataFrame corresponds to the names of the performance metrics\n",
    "    cv_scores.index = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 score\", \"ROC AUC\", \"CM\"]\n",
    "    \n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [logisticRegrFT, xgbClassFT]\n",
    "names = [\"Logistic regression\", \"XGBoost classifier\"]\n",
    "final_scores = model_comparison(models, names, x_test_ht, y_test_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic regression</th>\n",
       "      <th>XGBoost classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.817333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.678726</td>\n",
       "      <td>0.66571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.353429</td>\n",
       "      <td>0.349661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score</th>\n",
       "      <td>0.464817</td>\n",
       "      <td>0.458498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.652961</td>\n",
       "      <td>0.6499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CM</th>\n",
       "      <td>[[4451, 858], [222, 469]]</td>\n",
       "      <td>[[4440, 863], [233, 464]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Logistic regression         XGBoost classifier\n",
       "Accuracy                        0.82                   0.817333\n",
       "Precision                   0.678726                    0.66571\n",
       "Recall                      0.353429                   0.349661\n",
       "F1 score                    0.464817                   0.458498\n",
       "ROC AUC                     0.652961                     0.6499\n",
       "CM         [[4451, 858], [222, 469]]  [[4440, 863], [233, 464]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's check the results\n",
    "\n",
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Lets's plot the confusion matrices\n",
    "\n",
    "# We extract the confusion matrices from the DataFrame\n",
    "cm1 = final_scores.iloc[5, 0]\n",
    "cm2 = final_scores.iloc[5, 1]\n",
    "\n",
    "# We plot them\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 5))\n",
    "sns.heatmap(cm1, annot = True, fmt = \".3f\", linewidths = .5, square = True, cmap = \"Blues_r\", ax = ax[0])\n",
    "ax[0].set_title(\"Logistic regression\", size = 15)\n",
    "sns.heatmap(cm2, annot = True, fmt = \".3f\", linewidths = .5, square = True, cmap = \"Blues_r\", ax = ax[1])\n",
    "ax[1].set_title(\"XGBoost classifier\", size = 15)\n",
    "\n",
    "# Set common labels\n",
    "    # https://stackoverflow.com/questions/6963035/how-to-set-common-axes-labels-for-subplots\n",
    "plt.setp(ax[:], xlabel = \"Actual label\")\n",
    "plt.setp(ax[0], ylabel = \"Predicted label\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Summary\n",
    "\n",
    "Overall, both models perform in a similar way and rather poorly. The confusion matrix metrics tell us both models are rather poor. In particular, the recall scores indicate both of them have a high rate of false negatives. Differences across the performance metrics between the models are pretty consistent, with the logistic regression slightly ourperforming the XGBoost classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Concluding remarks\n",
    "\n",
    "This modeling exercise delved into a model selection process, going from a train/validation/test approach to a hyperparamteter tuning using cross-validation. While we only applied this analysis for a logistic regression and a XGBoost classifier, it can be extended to other classification models such as a random forest and a neural network.\n",
    "\n",
    "While overall the logistic regression outperformed the GXBoost classifier, the performance difference is rather small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14a8ab31f43739f8c61e31f0a764b4b9450b359fd4dd373e840d3db42f338d08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
